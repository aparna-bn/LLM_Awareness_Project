# python global_news_tracker.py
import feedparser
import sqlite3
import datetime
import time
import hashlib
import re # For regular expressions, used in text cleaning
from collections import defaultdict, Counter # For grouping and counting words
from sklearn.feature_extraction.text import TfidfVectorizer # For more sophisticated keyword extraction
from sklearn.metrics.pairwise import cosine_similarity # For comparing similarity between articles
import os # For file system operations (e.g., deleting database file)
import tkinter as tk # New: For GUI development
from tkinter import ttk, scrolledtext, messagebox # New: Tkinter widgets
import webbrowser # New: For opening URLs in a web browser
import requests  # For Gemini API calls
import json      # For Gemini API calls
from dotenv import load_dotenv  # For loading .env file

# --- Load environment variables from .env file ---
load_dotenv()

# --- Configuration ---
DATABASE_NAME = 'global_news_topics.db'
GOOGLE_NEWS_RSS_FEEDS = {
    "Technology": "https://news.google.com/rss/search?q=technology&hl=en-US&gl=US&ceid=US:en",
    "Global Economy": "https://news.google.com/rss/search?q=global+economy&hl=en-US&gl=US&ceid=US:en",
    "Climate Change": "https://news.google.com/rss/search?q=climate+change&hl=en-US&gl=US&ceid=US:en",
    "Politics": "https://news.google.com/rss/search?q=politics&hl=en-US&gl=US&ceid=US:en",
    "Health": "https://news.google.com/rss/rss/search?q=health&hl=en-US&gl=US&ceid=US:en"
    # You can add more categories and their corresponding RSS feed URLs here
}

# --- Database Functions ---

def connect_db():
    """Establishes a connection to the SQLite database."""
    try:
        conn = sqlite3.connect(DATABASE_NAME)
        conn.row_factory = sqlite3.Row # Allows accessing columns by name
        return conn
    except sqlite3.Error as e:
        print(f"Database connection error: {e}")
        return None

def create_tables(conn):
    """
    Creates the 'articles' table and the new 'topics' table in the database.
    Drops tables first to ensure schema updates are applied during development.
    """
    cursor = conn.cursor()
    # Drop tables if they exist to ensure schema updates are always applied during development
    cursor.execute('DROP TABLE IF EXISTS articles')
    cursor.execute('DROP TABLE IF EXISTS topics')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            id TEXT PRIMARY KEY,       -- Unique ID based on article link
            title TEXT NOT NULL,
            link TEXT UNIQUE NOT NULL, -- Ensures no duplicate articles
            published TEXT,            -- Stored as ISO format string
            summary TEXT,
            source TEXT,
            ingestion_date TEXT,       -- When the article was added to our DB
            cleaned_text TEXT,         -- Cleaned text for processing
            topic_id TEXT,             -- ID of the identified topic
            FOREIGN KEY (topic_id) REFERENCES topics(topic_id) -- Link to topics table
        )
    ''')
    
    # New table for topics
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS topics (
            topic_id TEXT PRIMARY KEY,    -- Unique ID for the topic
            llm_summary TEXT,             -- Summary generated by LLM
            last_updated TEXT             -- When the topic summary was last generated
        )
    ''')
    conn.commit()
    print(f"Tables 'articles' and 'topics' ensured in {DATABASE_NAME}")

def insert_article(conn, article_data):
    """Inserts a single article into the database if it doesn't already exist.
    Article data should be a dictionary with keys matching table columns.
    """
    cursor = conn.cursor()
    article_id = hashlib.sha256(article_data['link'].encode('utf-8')).hexdigest()

    try:
        cursor.execute('''
            INSERT OR IGNORE INTO articles (id, title, link, published, summary, source, ingestion_date, cleaned_text, topic_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            article_id,
            article_data['title'],
            article_data['link'],
            article_data['published'],
            article_data['summary'],
            article_data['source'],
            article_data['ingestion_date'],
            article_data.get('cleaned_text', ''),
            None # topic_id is initially NULL
        ))
        if cursor.rowcount > 0:
            return True
        else:
            return False
    except sqlite3.Error as e:
        print(f"Error inserting article '{article_data.get('title', 'N/A')}': {e}")
        return False

def update_article_processed_data(conn, article_id, cleaned_text, topic_id=None):
    """Updates an article with its cleaned text and identified topic ID."""
    cursor = conn.cursor()
    try:
        if topic_id:
            cursor.execute('''
                UPDATE articles
                SET cleaned_text = ?, topic_id = ?
                WHERE id = ?
            ''', (cleaned_text, topic_id, article_id))
        else:
            cursor.execute('''
                UPDATE articles
                SET cleaned_text = ?
                WHERE id = ?
            ''', (cleaned_text, article_id))
        conn.commit()
    except sqlite3.Error as e:
        print(f"Error updating article {article_id}: {e}")

def insert_or_update_topic_summary(conn, topic_id, llm_summary):
    """Inserts or updates a topic summary in the 'topics' table."""
    cursor = conn.cursor()
    last_updated = datetime.datetime.now().isoformat()
    try:
        cursor.execute('''
            INSERT OR REPLACE INTO topics (topic_id, llm_summary, last_updated)
            VALUES (?, ?, ?)
        ''', (topic_id, llm_summary, last_updated))
        conn.commit()
    except sqlite3.Error as e:
        print(f"Error inserting/updating topic summary for {topic_id}: {e}")

def get_articles_for_processing(conn):
    """Retrieves articles that have not yet been grouped (topic_id is NULL)."""
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, title, summary, link FROM articles WHERE topic_id IS NULL
    ''')
    articles = cursor.fetchall()
    print(f"DEBUG: Articles with topic_id IS NULL (from get_articles_for_processing query): {len(articles)}")
    return articles

def get_grouped_articles(conn):
    """Retrieves all articles grouped by their topic_id."""
    cursor = conn.cursor()
    cursor.execute('''
        SELECT topic_id, id, title, summary, cleaned_text
        FROM articles
        WHERE topic_id IS NOT NULL
        ORDER BY topic_id, published DESC
    ''')
    return cursor.fetchall()

def get_all_topics_with_summaries(conn):
    """Retrieves all topics and their LLM summaries."""
    cursor = conn.cursor()
    cursor.execute('''
        SELECT topic_id, llm_summary, last_updated
        FROM topics
        ORDER BY last_updated DESC
    ''')
    return cursor.fetchall()

# --- Data Fetching Functions ---

def fetch_rss_feed(feed_url):
    """Fetches and parses an RSS feed from the given URL."""
    try:
        d = feedparser.parse(feed_url, agent='GlobalNewsTopicTracker/1.0 (+http://yourwebsite.com/contact)')
        if d.bozo:
            print(f"Warning: Malformed RSS feed from {feed_url} - {d.bozo_exception}")
        return d.entries
    except Exception as e:
        print(f"Error fetching RSS feed from {feed_url}: {e}")
        return []

def process_feed_entries(entries, category):
    """Processes fetched RSS entries and formats them for database storage."""
    articles_to_insert = []
    ingestion_time = datetime.datetime.now().isoformat()

    for entry in entries:
        title = entry.title if hasattr(entry, 'title') else "No Title"
        link = entry.link if hasattr(entry, 'link') else None
        published_raw = entry.published if hasattr(entry, 'published') else None
        summary = entry.summary if hasattr(entry, 'summary') else "No Summary"
        source = entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else category

        if not link:
            print(f"Skipping article with no link in category {category}: {title}")
            continue

        published_iso = None
        if published_raw:
            try:
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_iso = datetime.datetime(*entry.published_parsed[:6]).isoformat()
                else:
                    published_iso = published_raw
            except Exception as e:
                print(f"Could not parse published date '{published_raw}': {e}. Storing raw.")
                published_iso = published_raw

        article_data = {
            'title': title,
            'link': link,
            'published': published_iso,
            'summary': summary,
            'source': source,
            'ingestion_date': ingestion_time,
            'cleaned_text': '', # Initialize empty string here for the new columns
            'topic_id': None # Initialize as None (NULL in SQLite)
        }
        articles_to_insert.append(article_data)
    return articles_to_insert

# --- Sprint 1 Main Function ---
def run_sprint1(conn): # Now accepts connection as an argument
    """Main function to execute Sprint 1 activities."""
    

    # create_tables(conn) # This is now called in the main execution block

    total_articles_fetched = 0
    total_articles_inserted = 0

    # 2. Iterate through RSS feeds, fetch, and store data
    for category, url in GOOGLE_NEWS_RSS_FEEDS.items():
        print(f"\nFetching news for category: {category} from {url}")
        entries = fetch_rss_feed(url)
        print(f"Found {len(entries)} entries for {category}.")
        total_articles_fetched += len(entries)

        articles_to_db = process_feed_entries(entries, category)
        print(f"Prepared {len(articles_to_db)} articles for insertion from {category}.")

        for article in articles_to_db:
            if insert_article(conn, article):
                total_articles_inserted += 1
                # print(f"  Inserted: {article['title']}") # Uncomment for detailed output
            # else:
                # print(f"  Skipped (already exists): {article['title']}") # Uncomment for detailed output

        # Optional: Be polite and pause between requests to avoid overwhelming servers
        # time.sleep(1) # Pause for 1 second between different feed fetches

    print(f"Total articles fetched across all feeds: {total_articles_fetched}")
    print(f"Total new articles inserted into database: {total_articles_inserted}")
    print(f"You can now inspect your '{DATABASE_NAME}' file (though it's still open).")


# --- Functions for Sprint 2 ---

def clean_text(text):
    """
    Cleans the input text by:
    1. Removing HTML tags.
    2. Removing special characters and numbers (keeping only letters and spaces).
    3. Converting to lowercase.
    4. Removing extra whitespace.
    """
    if not isinstance(text, str):
        return "" # Ensure text is a string
    # Remove HTML tags
    clean = re.compile('<.*?>')
    text = re.sub(clean, '', text)
    # Remove non-alphanumeric characters and numbers, replace with space
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def group_articles_by_keyword_similarity(articles, threshold=0.1): # Adjusted threshold here
    """
    Groups articles based on the cosine similarity of their TF-IDF vectors.
    This provides a more robust grouping than simple keyword overlap.
    Articles are assigned a topic_id.
    """
    if not articles:
        return {}

    # Combine title and summary for better representation
    documents = [f"{article['title']} {article['summary']}" for article in articles]
    article_ids = [article['id'] for article in articles]

    # Initialize TF-IDF Vectorizer
    # min_df ignores words that appear in less than 2 documents
    # stop_words removes common words that don't add much meaning
    vectorizer = TfidfVectorizer(stop_words='english', min_df=2)

    try:
        tfidf_matrix = vectorizer.fit_transform(documents)
    except ValueError as e:
        print(f"Could not create TF-IDF matrix: {e}. Not enough unique words after filtering? Returning empty groups.")
        return {}

    # Calculate cosine similarity between all articles
    cosine_sim_matrix = cosine_similarity(tfidf_matrix)

    # Initialize groups
    # {topic_id: [article_id1, article_id2, ...]}
    topics = {}
    # {article_id: topic_id}
    article_to_topic_map = {}
    next_topic_id = 0

    for i, article_id in enumerate(article_ids):
        if article_id not in article_to_topic_map:
            # This article hasn't been assigned to a topic yet, create a new one
            current_topic_id = f"topic_{next_topic_id}"
            next_topic_id += 1
            topics[current_topic_id] = [article_id]
            article_to_topic_map[article_id] = current_topic_id

            # Find similar articles and add them to this topic
            for j in range(i + 1, len(article_ids)):
                other_article_id = article_ids[j]
                if other_article_id not in article_to_topic_map:
                    similarity = cosine_sim_matrix[i, j]
                    if similarity >= threshold:
                        topics[current_topic_id].append(other_article_id)
                        article_to_topic_map[other_article_id] = current_topic_id
        
    # Filter out very small topics (e.g., topics with only one article)
    filtered_topics = {
        topic_id: article_ids_in_topic for topic_id, article_ids_in_topic in topics.items()
        if len(article_ids_in_topic) > 1 # Only consider topics with more than 1 article
    }
    
    # Re-map articles to filtered topics
    final_article_to_topic_map = {}
    for topic_id, article_ids_in_topic in filtered_topics.items():
        for art_id in article_ids_in_topic:
            final_article_to_topic_map[art_id] = topic_id
            
    return final_article_to_topic_map


def run_sprint2(conn): # Now accepts connection as an argument
    """Main function to execute Sprint 2 activities."""
    # --- Part 1: Data Preprocessing ---
    print("\n--- Part 1: Data Preprocessing (Cleaning Text) ---")
    articles_to_clean = get_articles_for_processing(conn)
    print(f"Found {len(articles_to_clean)} articles to clean and process.")

    cleaned_articles_data = []
    for article in articles_to_clean:
        # Combine title and summary for cleaning, as both contain relevant info
        full_text = f"{article['title']} {article['summary']}"
        cleaned_text = clean_text(full_text)
        cleaned_articles_data.append({
            'id': article['id'],
            'title': article['title'], # Keep original title
            'summary': article['summary'], # Keep original summary
            'link': article['link'], # Keep original link
            'cleaned_text': cleaned_text
        })
        # Update the cleaned_text in the database immediately
        update_article_processed_data(conn, article['id'], cleaned_text)
    print(f"Cleaned text for {len(cleaned_articles_data)} articles.")

    # --- Part 2: Initial Topic Grouping ---
    print("\n--- Part 2: Initial Topic Grouping (Keyword Similarity) ---")

    # Filter out articles that have empty cleaned_text, as they won't contribute to similarity
    processable_articles = [art for art in cleaned_articles_data if art['cleaned_text']]
    
    # The group_articles_by_keyword_similarity function expects a list of dictionaries with 'id', 'title', 'summary'
    # For TFIDF, the 'summary' field should contain the text we want to analyze, which is our 'cleaned_text'.
    # We create a temporary list structure for this.
    temp_articles_for_tfidf = [
        {'id': art['id'], 'title': art['title'], 'summary': art['cleaned_text']}
        for art in processable_articles
    ]
    
    # Perform grouping
    article_to_topic_map = group_articles_by_keyword_similarity(temp_articles_for_tfidf, threshold=0.1) # Adjusted threshold here

    grouped_topics_summary = defaultdict(list)
    for article_id, topic_id in article_to_topic_map.items():
        grouped_topics_summary[topic_id].append(article_id)
        # Update the database with the topic_id for each article
        update_article_processed_data(conn, article_id, # Use article_id directly from the map
                                     next(item for item in processable_articles if item["id"] == article_id)['cleaned_text'], # Get the cleaned text back
                                     topic_id)

    print(f"\nIdentified {len(grouped_topics_summary)} potential topics.")
    print("--- Topics Summary ---")
    for topic_id, article_ids in grouped_topics_summary.items():
        print(f"  Topic {topic_id}: {len(article_ids)} articles")
        # Optional: Print some article titles from the topic for verification
        # for article_id in article_ids[:3]: # Print first 3 article titles
        #     # Retrieve article title from articles_to_clean list
        #     original_article = next((a for a in articles_to_clean if a['id'] == article_id), None)
        #     if original_article:
        #         print(f"    - {original_article['title']}")

    # conn.close() # Connection will be closed by the main block
    print(f"Articles cleaned and initial topic grouping performed based on keyword similarity.")
    print(f"You can inspect the 'cleaned_text' and 'topic_id' columns in your '{DATABASE_NAME}' file.")

def get_llm_summary(text_to_summarize):
    """
    Uses Google Gemini 2.0 Flash model to summarize the given text.
    Requires GEMINI_API_KEY to be set in a .env file.
    Includes retry logic for rate limits.
    """
    if not text_to_summarize.strip():
        return "No content to summarize."

    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print("Error: GEMINI_API_KEY not set in .env file.")
        return "Error: Gemini API key not set."

    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"

    prompt = (
        "You are an expert news analyst.\n"
        "Your task is to provide a highly precise and concise summary of the following collection of news articles.\n\n"
        "Focus on:\n"
        "1. The central event or development.\n"
        "2. Key actors or entities involved.\n"
        "3. The immediate impact or significance.\n"
        "4. Any notable quotes or statistics (if present).\n\n"
        "Generate a single paragraph summary, no more than 150 words, based ONLY on the provided text. Do not add external information.\n\n"
        f"Articles:\n{text_to_summarize}"
    )

    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.2, # Lower temperature for more focused output
            "maxOutputTokens": 150 # Limit output length to 150 words
        }
    }

    headers = {'Content-Type': 'application/json'}
    max_retries = 3
    for retry_count in range(max_retries):
        try:
            response = requests.post(api_url, headers=headers, data=json.dumps(payload), timeout=30)
            if response.status_code == 429:
                sleep_time = 2 ** (retry_count + 1) # Exponential backoff: 2, 4, 8 seconds
                print(f"Gemini API rate limit reached (429). Retrying in {sleep_time} seconds (Attempt {retry_count + 1}/{max_retries})...")
                time.sleep(sleep_time)
                continue # Try the request again
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
            result = response.json()
            
            if result and "candidates" in result and result["candidates"]:
                parts = result["candidates"][0].get("content", {}).get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]
                else:
                    return "LLM returned no valid summary."
            else:
                return "LLM returned no valid summary."
        except requests.exceptions.RequestException as e:
            print(f"LLM API call failed: {e}")
            if retry_count < max_retries - 1:
                sleep_time = 2 ** (retry_count + 1)
                print(f"Retrying in {sleep_time} seconds (Attempt {retry_count + 1}/{max_retries})...")
                time.sleep(sleep_time)
            else:
                return f"Error generating summary after {max_retries} retries: {e}"
    return "Failed to generate summary after multiple retries." # Should not be reached if max_retries is 0 or more


def run_sprint3(conn):
    """Main function to execute Sprint 3 activities: LLM summarization of topics."""

    print("\n--- Part 1: Summarizing Identified Topics with LLMs ---")
    grouped_articles_data = get_grouped_articles(conn)

    # Reorganize articles by topic_id
    topics_to_summarize = defaultdict(list)
    for article in grouped_articles_data:
        if article['topic_id']: # Only process articles that were successfully grouped
            topics_to_summarize[article['topic_id']].append(article)

    print(f"Found {len(topics_to_summarize)} topics to summarize.")

    summarized_topics_count = 0
    # Limit to a small number of topics for demonstration to avoid rate limits
    MAX_DEMO_TOPICS = 10 # Increased to 10 topics for a richer demo
    topics_to_process = list(topics_to_summarize.items())[:MAX_DEMO_TOPICS]

    for topic_id, articles_in_topic in topics_to_process:
        # Collect relevant text for summarization
        # Prioritize cleaned_text, fallback to title + summary
        text_for_llm = ""
        for article in articles_in_topic:
            if article['cleaned_text']:
                text_for_llm += article['cleaned_text'] + "\n"
            else:
                text_for_llm += f"{article['title']} {article['summary']}\n"
        
        # Limit text length for LLM if it's too long (important for real APIs)
        MAX_LLM_INPUT_LENGTH = 3000 # Example limit, adjust based on LLM token limits
        if len(text_for_llm) > MAX_LLM_INPUT_LENGTH:
            text_for_llm = text_for_llm[:MAX_LLM_INPUT_LENGTH] + "..." # Truncate
            print(f"Warning: Truncated text for topic {topic_id} due to length limit.")

        if text_for_llm.strip():
            llm_summary = get_llm_summary(text_for_llm)
            insert_or_update_topic_summary(conn, topic_id, llm_summary)
            summarized_topics_count += 1
            print(f"  Summarized Topic {topic_id}: {llm_summary[:100]}...") # Print first 100 chars
            time.sleep(5)  # Increased delay for API rate limits
        else:
            print(f"  Skipped Topic {topic_id}: No sufficient text to summarize.")

    print(f"\nSuccessfully generated LLM summaries for {summarized_topics_count} topics.")

    print(f"Topic summaries generated and stored in the 'topics' table of '{DATABASE_NAME}'.")


def display_gui_dashboard(conn):
    """Main function to execute Sprint 5 activities: Presenting topics and summaries in a GUI."""
    root = tk.Tk()
    root.title("Global News Topic Tracker")
    root.geometry("800x600") # Set initial window size

    # Configure grid weights for responsive layout
    root.grid_rowconfigure(0, weight=1)
    root.grid_columnconfigure(0, weight=1)

    # Main frame for content
    main_frame = ttk.Frame(root, padding="10")
    main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
    main_frame.grid_rowconfigure(1, weight=1) # Listbox/ScrolledText area
    main_frame.grid_columnconfigure(0, weight=1)

    # Title Label
    title_label = ttk.Label(main_frame, text="Global News Trending Topics", font=("Helvetica", 16, "bold"))
    title_label.grid(row=0, column=0, pady=10, sticky=tk.W)

    # Scrolled Text for displaying topics
    topics_display = scrolledtext.ScrolledText(main_frame, wrap=tk.WORD, width=80, height=25, font=("Helvetica", 10))
    topics_display.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=5)
    topics_display.insert(tk.END, "Loading trending topics...\n")
    topics_display.config(state=tk.DISABLED) # Make it read-only

    # Define text tags for formatting
    topics_display.tag_configure("topic_title", font=("Helvetica", 14, "bold"), foreground="dark blue", spacing3=6)      # Larger, bold, navy
    topics_display.tag_configure("summary", font=("Arial", 12), foreground="black", spacing3=6)                     # Medium, black
    topics_display.tag_configure("metadata", font=("Arial", 10, "italic"), foreground="gray", spacing3=4)           # Small, italic, gray
    topics_display.tag_configure("article_title", font=("Helvetica", 11, "bold"), foreground="darkgreen", spacing3=4) # Medium, bold, green
    topics_display.tag_configure("article_link", font=("Arial", 10, "underline"), foreground="blue", spacing3=4)    # Small, underline, blue
    topics_display.tag_configure("separator", foreground="lightgray", spacing3=8)

    # Store URLs in a dictionary, mapped to their start index in the text widget
    # This allows us to retrieve the URL when a link is clicked.
    url_map = {}

    def on_link_click(event):
        """Event handler for clicking on a link in the ScrolledText widget."""
        index = topics_display.index(f"@{event.x},{event.y}")
        # Get all tags at the clicked index
        tags = topics_display.tag_names(index)
        for tag in tags:
            if tag.startswith("link_"):
                # Extract the URL from the tag name (e.g., "link_http://example.com")
                url = tag[len("link_"):]
                webbrowser.open_new_tab(url)
                return

    # Bind the click event to the text widget
    topics_display.bind("<Button-1>", on_link_click)
    # Change cursor to hand when hovering over a link
    topics_display.tag_bind("article_link", "<Enter>", lambda e: topics_display.config(cursor="hand2"))
    topics_display.tag_bind("article_link", "<Leave>", lambda e: topics_display.config(cursor="arrow"))


    def load_and_display_topics(db_conn_for_display): # Accepts connection as argument
        """Fetches topics from DB and updates the GUI display with enhanced formatting."""
        topics_display.config(state=tk.NORMAL)
        topics_display.delete(1.0, tk.END) # Clear previous content
        url_map.clear() # Clear previous URL map

        topics = get_all_topics_with_summaries(db_conn_for_display) # Use the passed connection
        
        if not topics:
            topics_display.insert(tk.END, "No trending topics found. Click 'Run Pipeline' to fetch and process news.\n")
        else:
            for i, topic in enumerate(topics):
                # Topic Title (remove topic_id from display)
                topics_display.insert(tk.END, f"Topic {i+1}\n", "topic_title")

                # Summary
                topics_display.insert(tk.END, f"  Summary: ", "summary")
                topics_display.insert(tk.END, f"{topic['llm_summary']}\n", "summary")

                # Last Updated
                topics_display.insert(tk.END, f"  Last Updated: ", "metadata")
                topics_display.insert(tk.END, f"{topic['last_updated']}\n", "metadata")
                
                # Fetch articles for this specific topic
                cursor = db_conn_for_display.cursor() # Use the passed connection
                cursor.execute('''
                    SELECT title, link
                    FROM articles
                    WHERE topic_id = ?
                    ORDER BY published DESC
                    LIMIT 3 -- Show top 3 most recent articles for the topic
                ''', (topic['topic_id'],))
                related_articles = cursor.fetchall()

                if related_articles:
                    topics_display.insert(tk.END, "  Related Articles (Top 3):\n", "article_title")
                    for article in related_articles:
                        topics_display.insert(tk.END, f"    - ", "article_title")
                        topics_display.insert(tk.END, f"{article['title']}\n", "article_title")
                        
                        # Insert link and apply both 'article_link' tag and a unique tag for the URL
                        link_text = f"      ({article['link']})\n"
                        # The unique tag is created on the fly as "link_URL"
                        topics_display.insert(tk.END, link_text, ("article_link", f"link_{article['link']}"))

                else:
                    topics_display.insert(tk.END, "  No related articles found for this topic.\n", "metadata")
                
                topics_display.insert(tk.END, "-" * 70 + "\n\n", "separator") # Separator for readability
    
        topics_display.config(state=tk.DISABLED)

    def run_full_pipeline_and_refresh_gui():
        """Runs all sprints and then refreshes the GUI."""
        messagebox.showinfo("Processing", "Starting news pipeline. This may take a moment...")
        
        # Disable button during processing
        run_button.config(state=tk.DISABLED)
        
        # Ensure a clean database file before starting
        if os.path.exists(DATABASE_NAME):
            os.remove(DATABASE_NAME)
            print(f"Removed existing database file: {DATABASE_NAME}")

        # Establish a single connection for all sprints within this run
        # This connection will be passed to sprints and then used to refresh GUI
        pipeline_conn = connect_db() # Create new connection for pipeline run
        if not pipeline_conn:
            messagebox.showerror("Error", "Fatal: Could not establish database connection for pipeline.")
            root.destroy()
            return
        
        try:
            # Create tables only once at the start of the pipeline run
            create_tables(pipeline_conn) 
            run_sprint1(pipeline_conn)
            run_sprint2(pipeline_conn)
            run_sprint3(pipeline_conn)
            
            # Now, refresh GUI using the *same* connection that just wrote the data
            load_and_display_topics(pipeline_conn) # Pass the pipeline_conn
            messagebox.showinfo("Success", "News pipeline completed and dashboard updated!")
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred during pipeline execution: {e}")
        finally:
            pipeline_conn.close() # Close the pipeline-specific connection
            run_button.config(state=tk.NORMAL) # Re-enable button

    # Run Pipeline Button
    run_button = ttk.Button(main_frame, text="Run Pipeline & Refresh", command=run_full_pipeline_and_refresh_gui)
    run_button.grid(row=2, column=0, pady=10)

    # Initial load of topics (using the main_conn)
    # This will display "No trending topics found" initially because the DB is clean
    load_and_display_topics(conn) # Pass the main_conn here for initial display

    root.mainloop() # Start the Tkinter event loop

# --- Main Execution Flow ---

if __name__ == "__main__":
    # Ensure a clean database file before starting
    if os.path.exists(DATABASE_NAME):
        os.remove(DATABASE_NAME)
        print(f"Removed existing database file: {DATABASE_NAME}")

    # Establish a single connection for the GUI's initial display.
    # The pipeline run will create its own temporary connection.
    main_conn = connect_db()
    if not main_conn:
        print("Fatal: Could not establish main database connection. Exiting.")
    else:
        # Create tables immediately after main_conn is established for initial GUI load.
        # This ensures the 'topics' table exists when load_and_display_topics is first called.
        create_tables(main_conn) 

        # Launch the GUI. The GUI's "Run Pipeline" button will trigger the full sequence.
        display_gui_dashboard(main_conn)

        # Close the main connection when the GUI application closes
        main_conn.close()
        print(f"\nMain database connection to {DATABASE_NAME} closed.")
